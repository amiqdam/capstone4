{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhjHqh-DO5rn"
      },
      "source": [
        "Sign Language Project Computer Vision Dataset\n",
        "\n",
        "https://universe.roboflow.com/sign-language-colorful/sign-language-project-zxbft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_2cu4nMIjWL",
        "outputId": "b9570903-b929-40b5-e0b2-31d02765ea39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Users/ahmadmiqdam/Desktop/capstone4/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "%pip install -q ultralytics roboflow supervision pandas matplotlib opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GK065ypMPETT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ahmadmiqdam/Desktop/capstone4/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Install library\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from roboflow import Roboflow\n",
        "from ultralytics import YOLO\n",
        "import shutil\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZZafmofQWStb"
      },
      "outputs": [],
      "source": [
        "# Load API required\n",
        "load_dotenv()\n",
        "# Import Roboflow API\n",
        "ROBOFLOW_API_KEY = os.environ[\"ROBOFLOW_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtGdLNbjPFXh",
        "outputId": "56f578d2-87d4-470e-80a0-0ee82dff97b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ],
      "source": [
        "# Load first dataset (Object Detection)\n",
        "\n",
        "rf_1 = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "project_1 = rf_1.workspace(\"test-hmtgo\").project(\"sign-language-project-zxbft-ekfrd\")\n",
        "version_1 = project_1.version(1)\n",
        "dataset_1 = version_1.download(\"yolov8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s6N_dsl7Rkxb"
      },
      "outputs": [],
      "source": [
        "# Set out directory for object detection\n",
        "\n",
        "HOME = os.getcwd()\n",
        "base_path = Path(HOME)\n",
        "first_dataset_location = base_path / \"Sign-Language-Project-1\"\n",
        "yaml_data = first_dataset_location / \"data.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_J0w1S4E2L1B"
      },
      "outputs": [],
      "source": [
        "# Function for restructuring data.yaml so it would convert from 36 class into 1 class (hand) only\n",
        "\n",
        "def convert_yolo_labels_to_single_class(\n",
        "    dataset_root: str,\n",
        "    target_class_id: int = 0,\n",
        "    dry_run: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Convert all YOLO label files in train/valid/test splits to a single class.\n",
        "    \"\"\"\n",
        "    dataset_root = Path(dataset_root)\n",
        "    splits = [\"train\", \"valid\", \"test\"]\n",
        "\n",
        "    for split in splits:\n",
        "        labels_dir = dataset_root / split / \"labels\"\n",
        "        if not labels_dir.exists():\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nProcessing {labels_dir}\")\n",
        "\n",
        "        for label_file in labels_dir.glob(\"*.txt\"):\n",
        "            with open(label_file, \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            new_lines = []\n",
        "            changed = False\n",
        "\n",
        "            for line in lines:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) != 5:\n",
        "                    continue\n",
        "\n",
        "                if parts[0] != str(target_class_id):\n",
        "                    parts[0] = str(target_class_id)\n",
        "                    changed = True\n",
        "\n",
        "                new_lines.append(\" \".join(parts))\n",
        "\n",
        "            if changed:\n",
        "                if dry_run:\n",
        "                    print(f\"Dry Run Would update {label_file.name}\")\n",
        "                else:\n",
        "                    with open(label_file, \"w\") as f:\n",
        "                        f.write(\"\\n\".join(new_lines))\n",
        "                    print(f\"Updated {label_file.name}\")\n",
        "\n",
        "\n",
        "def update_data_yaml_to_single_class(\n",
        "    yaml_path: str,\n",
        "    class_name: str = \"hand\"\n",
        "):\n",
        "    with open(yaml_path, \"r\") as f:\n",
        "        data = yaml.safe_load(f)\n",
        "\n",
        "    data[\"nc\"] = 1\n",
        "    data[\"names\"] = {0: class_name}\n",
        "\n",
        "    with open(yaml_path, \"w\") as f:\n",
        "        yaml.safe_dump(data, f)\n",
        "\n",
        "    print(f\"Updated {yaml_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Bd8ZlH7y2fzE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing Sign-Language-Project-1/train/labels\n",
            "\n",
            "Processing Sign-Language-Project-1/valid/labels\n",
            "\n",
            "Processing Sign-Language-Project-1/test/labels\n",
            "Updated Sign-Language-Project-1/data.yaml\n"
          ]
        }
      ],
      "source": [
        "# Convert all label files to class 0\n",
        "convert_yolo_labels_to_single_class(\n",
        "    dataset_root=\"Sign-Language-Project-1\",\n",
        "    target_class_id=0\n",
        ")\n",
        "# Convert data.yaml to 1-class\n",
        "update_data_yaml_to_single_class(\n",
        "    yaml_path=\"Sign-Language-Project-1/data.yaml\",\n",
        "    class_name=\"hand\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WROr5itPKBC",
        "outputId": "b951fae1-cbc4-4226-e816-7a15e67564d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvidia-smi\n"
          ]
        }
      ],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP_UzWlG0WyI"
      },
      "source": [
        "Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIRkjFCKVkL4",
        "outputId": "4db179e5-30ab-4c56-dd1b-e1e9dc1a8e9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2026-01-15 15:46:37--  https://universe.roboflow.com/test-hmtgo/handsign-5nz1l-ehtlc\n",
            "Resolving universe.roboflow.com (universe.roboflow.com)... 172.66.166.205, 104.20.41.123, 2606:4700:10::ac42:a6cd, ...\n",
            "Connecting to universe.roboflow.com (universe.roboflow.com)|172.66.166.205|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2026-01-15 15:46:37 ERROR 403: Forbidden.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://universe.roboflow.com/test-hmtgo/handsign-5nz1l-ehtlc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dqvSjE0TdAr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.4.2 üöÄ Python-3.9.6 torch-2.8.0 CPU (Apple M4 Pro)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/Users/ahmadmiqdam/Desktop/capstone4/Sign-Language-Project-1/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=40, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=model_12, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/Users/ahmadmiqdam/visualcode/runs/detect/model_12, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, 16, None, [128, 256, 512]]\n",
            "Model summary: 130 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 36.2¬±15.3 MB/s, size: 10.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/ahmadmiqdam/Desktop/capstone4/Sign-Language-Project-1/train/labels.cache... 2880 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2880/2880 604.0Mit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 46.6¬±24.2 MB/s, size: 9.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/ahmadmiqdam/Desktop/capstone4/Sign-Language-Project-1/valid/labels.cache... 360 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 188.7Mit/s 0.0s\n",
            "Plotting labels to /Users/ahmadmiqdam/visualcode/runs/detect/model_12/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.0' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m MuSGD(lr=0.002, momentum=0.9) with parameter groups 0 weight(decay=0.0), 0 weight(decay=0.0005), 0 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/Users/ahmadmiqdam/visualcode/runs/detect/model_12\u001b[0m\n",
            "Starting training for 40 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/40         0G     0.9515      1.537       1.49         17        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 7.4s/it 44:31<2.6s06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.8s/it 1:273.9sss\n",
            "                   all        360        360      0.992      0.987      0.995      0.871\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/40         0G     0.6833     0.5552      1.209         20        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 4.2s/it 25:06<2.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.8s/it 1:284.0sss\n",
            "                   all        360        360      0.994      0.999      0.995      0.887\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/40         0G      0.635     0.4594      1.161         19        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.3s/it 19:55<1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:233.7sss\n",
            "                   all        360        360      0.994      0.996      0.995      0.902\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/40         0G     0.6347     0.4552      1.163         21        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.1s/it 18:20<1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:243.7sss\n",
            "                   all        360        360      0.994      0.997      0.995      0.916\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/40         0G     0.6156     0.4321      1.139         13        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.0s/it 18:03<1.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:233.7sss\n",
            "                   all        360        360      0.997          1      0.995      0.911\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/40         0G     0.6077     0.4271      1.138         11        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 2.9s/it 17:33<1.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:223.7sss\n",
            "                   all        360        360      0.999          1      0.995      0.914\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/40         0G     0.6035     0.4095      1.138         17        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.0s/it 18:02<1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:223.7sss\n",
            "                   all        360        360      0.999          1      0.995      0.922\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/40         0G     0.5787     0.3975      1.114         18        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.1s/it 18:31<1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:233.7sss\n",
            "                   all        360        360          1          1      0.995      0.929\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/40         0G      0.566     0.3804      1.102         24        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.1s/it 18:22<1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.5s/it 1:223.7sss\n",
            "                   all        360        360          1          1      0.995      0.941\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/40         0G     0.5499     0.3741      1.092         15        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.0s/it 18:10<1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:233.7sss\n",
            "                   all        360        360          1          1      0.995       0.93\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/40         0G     0.5583     0.3695        1.1         18        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.0s/it 18:02<1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:233.7sss\n",
            "                   all        360        360          1          1      0.995      0.939\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/40         0G     0.5484     0.3636      1.094         20        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.0s/it 17:51<1.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:223.7sss\n",
            "                   all        360        360          1          1      0.995      0.942\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/40         0G     0.5233     0.3434      1.076         23        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.0s/it 17:49<1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:233.7sss\n",
            "                   all        360        360          1          1      0.995      0.945\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/40         0G     0.5321     0.3455      1.081         19        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.0s/it 17:57<1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:233.7sss\n",
            "                   all        360        360          1          1      0.995      0.944\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/40         0G      0.519     0.3389      1.074         15        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.1s/it 18:21<1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:223.7sss\n",
            "                   all        360        360          1          1      0.995      0.946\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/40         0G     0.5222     0.3366      1.073         19        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.0s/it 18:15<2.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:223.7sss\n",
            "                   all        360        360          1          1      0.995      0.948\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/40         0G     0.4972     0.3248      1.053         19        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.0s/it 18:03<1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:223.7sss\n",
            "                   all        360        360          1          1      0.995      0.939\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/40         0G     0.4968     0.3202      1.055         18        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 3.0s/it 17:51<1.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23/23 3.6s/it 1:233.7sss\n",
            "                   all        360        360      0.997          1      0.995       0.95\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/40         0G     0.4904     0.3163      1.057         20        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 360/360 73.2s/it 7:18:573.7s5\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 61% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 14/23 14.7s/it 24:36<2:12"
          ]
        }
      ],
      "source": [
        "# Model Comparison, pick the best model later\n",
        "\n",
        "# 1. Base model\n",
        "# (Pretrained YOLOv8s with no learning, no finetuning and augmentation as baseline)\n",
        "model_1 = YOLO('yolov8s.pt')\n",
        "results_1 = model_1.train(\n",
        "    data=yaml_data,\n",
        "    epochs=40,\n",
        "    imgsz=640,\n",
        "    batch=8,\n",
        "    name='model_1',\n",
        "    lr0=0.0,\n",
        "    augment=False,\n",
        ")\n",
        "\n",
        "# 2. Fine-tuned\n",
        "# Fine tune only, no augmentation\n",
        "model_2 = YOLO('yolov8s.pt')\n",
        "results_2 = model_2.train(\n",
        "    data=yaml_data,\n",
        "    epochs=40,\n",
        "    imgsz=640,\n",
        "    batch=8,\n",
        "    name='model_2',\n",
        "    lr0=0.001,\n",
        "    augment=False,\n",
        ")\n",
        "\n",
        "# 3. Fine-tuned + no geometric augmentation\n",
        "model_3 = YOLO('yolov8s.pt')\n",
        "results_3 = model_3.train(\n",
        "    data=yaml_data,\n",
        "    epochs=40,\n",
        "    imgsz=640,\n",
        "    batch=8,\n",
        "    name='model_3',\n",
        "    lr0=0.001,\n",
        "    # Zero geometric augmentations\n",
        "    fliplr=0.0,\n",
        "    flipud=0.0,\n",
        "    mosaic=0.0,\n",
        "    degrees=0.0,\n",
        "    translate=0.0,\n",
        "    scale=0.0,\n",
        "    shear=0.0,\n",
        "    # Allowed photometric augmentations\n",
        "    hsv_h=0.01,\n",
        "    hsv_s=0.5,\n",
        "    hsv_v=0.4,\n",
        "    erasing=0.1,\n",
        ")\n",
        "\n",
        "# 4. Fine-tuned + geometric augmentation\n",
        "model_4 = YOLO('yolov8s.pt')\n",
        "results_4 = model_4.train(\n",
        "    data=yaml_data,\n",
        "    epochs=40,\n",
        "    imgsz=640,\n",
        "    batch=8,\n",
        "    name='model_4',\n",
        "    lr0=0.001,\n",
        "    fliplr=0.5,\n",
        "    degrees=15,\n",
        "    scale=0.5,\n",
        "    hsv_h=0.01,\n",
        "    hsv_s=0.5,\n",
        "    hsv_v=0.4,\n",
        "    erasing=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmN5BzmnTAId"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "all_metrics = []\n",
        "\n",
        "models = [\n",
        "    (\"Model 1 ‚Äì Base\", model_1, \"No fine-tune, no augmentation\"),\n",
        "    (\"Model 2 ‚Äì FT No Aug\", model_2, \"Fine-tune only\"),\n",
        "    (\"Model 3 ‚Äì FT Safe Aug\", model_3, \"Photometric augmentation\"),\n",
        "    (\"Model 4 ‚Äì FT Geo Aug\", model_4, \"Geometric augmentation\"),\n",
        "]\n",
        "\n",
        "for name, model, desc in models:\n",
        "    metrics = model.val()\n",
        "    metrics_result = metrics.results_dict\n",
        "\n",
        "    all_metrics.append({\n",
        "        \"Model\": name,\n",
        "        \"Description\": desc,\n",
        "        \"Precision\": metrics_result[\"metrics/precision(B)\"],\n",
        "        \"Recall\": metrics_result[\"metrics/recall(B)\"],\n",
        "        \"mAP50\": metrics_result[\"metrics/mAP50(B)\"],\n",
        "        \"mAP50-95\": metrics_result[\"metrics/mAP50-95(B)\"],\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(all_metrics)\n",
        "display(metrics_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a68-3qm27CUq"
      },
      "outputs": [],
      "source": [
        "# Download the best model (.pt)\n",
        "\n",
        "def export_all_trained_models(\n",
        "    runs_dir=\"runs/detect\",\n",
        "    model_names=(\"model_1\", \"model_2\", \"model_3\", \"model_4\"),\n",
        "    output_dir=\"exported_models\",\n",
        "    weight_name=\"best.pt\"\n",
        "):\n",
        "    runs_dir = Path(runs_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    exported = []\n",
        "\n",
        "    for model_name in model_names:\n",
        "        src = runs_dir / model_name / \"weights\" / weight_name\n",
        "        if not src.exists():\n",
        "            print(f\"[WARNING] {src} not found, skipping\")\n",
        "            continue\n",
        "\n",
        "        dst = output_dir / f\"{model_name}_{weight_name}\"\n",
        "        shutil.copy(src, dst)\n",
        "        exported.append(dst.name)\n",
        "        print(f\"Exported: {dst}\")\n",
        "\n",
        "    return exported"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e93CM0ZzWCTv"
      },
      "outputs": [],
      "source": [
        "exported_models = export_all_trained_models()\n",
        "print(\"Exported models:\", exported_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyBPEWMR5aun"
      },
      "outputs": [],
      "source": [
        "# So we will use the ... model because of blabla (download .pt nya)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq03GX9d0aFE"
      },
      "source": [
        "Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT5W4Cz_rIt_"
      },
      "outputs": [],
      "source": [
        "# Load second dataset (Image Classification)\n",
        "rf_2 = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "project_2 = rf_2.workspace(\"test-hmtgo\").project(\"handsign-5nz1l-ehtlc\")\n",
        "version_2 = project_2.version(1)\n",
        "dataset_2 = version_2.download(\"folder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DaJxxI_pzEM9"
      },
      "outputs": [],
      "source": [
        "# Set out directory for image classification\n",
        "\n",
        "HOME = os.getcwd()\n",
        "base_path = Path(HOME)\n",
        "second_dataset_location = base_path / \"HandSIgn-1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13IJBLvhzWXV",
        "outputId": "482328ce-4988-499f-892c-29880f1d6e4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "README.dataset.txt  README.roboflow.txt  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/  \u001b[01;34mvalid\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls {second_dataset_location}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8b_Xkwk0bqw"
      },
      "outputs": [],
      "source": [
        "# Load pretrained YOLOv8 classification backbone\n",
        "model_cls = YOLO(\"yolov8n-cls.pt\")\n",
        "\n",
        "results = model_cls.train(\n",
        "    data=second_dataset_location,   # dataset root\n",
        "    epochs=40,\n",
        "    imgsz=224,\n",
        "    batch=32,\n",
        "    name=\"handsign_cls_v1\",\n",
        "    lr0=0.001,\n",
        "    optimizer=\"Adam\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1i1FhNuW0-U"
      },
      "outputs": [],
      "source": [
        "# Validation\n",
        "metrics = model_cls.val()\n",
        "print(metrics.results_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCry5V_RXAvS"
      },
      "source": [
        "Webcam integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25yt-g5QXCL9"
      },
      "outputs": [],
      "source": [
        "model = YOLO(\"exported_models/handsign_cls_best.pt\")\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    res = model(frame, imgsz=224)[0]\n",
        "    probs = res.probs\n",
        "\n",
        "    label = model.names[probs.top1]\n",
        "    conf = probs.top1conf.item()\n",
        "\n",
        "    cv2.putText(\n",
        "        frame,\n",
        "        f\"{label} ({conf:.2f})\",\n",
        "        (20, 40),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1,\n",
        "        (0, 255, 0),\n",
        "        2\n",
        "    )\n",
        "\n",
        "    cv2.imshow(\"Hand Sign Classification\", frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elALjrYOXKyz"
      },
      "source": [
        "Streamlit / Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
